# Neural Network from Scratch

This repository contains a neural network implementation built from scratch using Python. The goal is to understand the inner workings of neural networks by creating one without relying on high-level libraries.

## Features

- Forward propagation
- Backward propagation
- Gradient descent optimization
- Support for multiple layers
- Activation functions: Sigmoid, ReLU, Tanh

## Requirements

- Python 3.x
- NumPy

## Installation

Clone the repository:
```bash
git clone https://github.com/Khalidabdulkadir/Neural-Networks-From-Scratch.git
```

Navigate to the project directory:
```bash
cd Advanced Algorithms
```

Install the required dependencies:
```bash
pip install -r requirements.txt
```

## Usage

To train the neural network, run:
```bash
python train.py
```

## Project Structure

## Project Structure

- `train.py`: Script to train the neural network.
- `network.py`: Contains the neural network implementation.
- `utils.py`: Utility functions for data preprocessing and other tasks.
- `activation_functions/`: Directory containing activation functions such as Sigmoid, ReLU, and Perceptron.
- `cost_functions/`: Directory containing cost function implementations.
- `gradient_descent/`: Directory containing gradient descent algorithm implementations.
- `data/`: Directory to store datasets.
## Contributing

Contributions are welcome! Please open an issue or submit a pull request.
